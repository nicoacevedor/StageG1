<?xml version="1.0" encoding="UTF-8" ?><ChoregrapheProject xmlns="http://www.aldebaran-robotics.com/schema/choregraphe/project.xsd" xar_version="3"><Box name="root" id="-1" localization="8" tooltip="Root box of Choregraphe&apos;s behavior. Highest level possible." x="0" y="0"><bitmap>media/images/box/root.png</bitmap><script language="4"><content><![CDATA[]]></content></script><Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" /><Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" /><Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" /><Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" /><Timeline enable="0"><BehaviorLayer name="behavior_layer1"><BehaviorKeyframe name="keyframe1" index="1"><Diagram scale="118.921"><Box name="Mask Detection" id="1" localization="8" tooltip="Programme pour la detection du masque&#x0A;" x="91" y="28"><bitmap>media/images/box/box-python-script.png</bitmap><script language="4"><content><![CDATA[#import cv2
#import os
#import tensorflow as tf
#import numpy as np

## Detection de visages à l'aide du model Cafee Model Zoo
## http://caffe.berkeleyvision.org/model_zoo.html
#prototxt_path = os.path.join('resources/deploy.prototxt')
#caffemodel_path = os.path.join('resources/weights.caffemodel')
#model = cv2.dnn.readNetFromCaffe(prototxt_path, caffemodel_path)

##Chargement du modèle permettant de détecter le port du masque
#modelMasque = tf.keras.models.load_model("resources/QSTOMIT-MASQUE.model")

class MyClass(GeneratedClass):
    def __init__(self):
        try: # disable autoBind
          GeneratedClass.__init__(self, False)
        except TypeError: # if NAOqi < 1.14
          GeneratedClass.__init__( self )
        self.camera = ALProxy("ALVideoDevice")
        self.tts = ALProxy("ALTextToSpeech")
        self.camera_top = 0

    def onLoad(self):
        # subscribe(cameraIndex, resolution, colorSpace, fps)
        # 0 -> camera top
        # 2 -> 480x640
        # 11 -> RGB
        self.camera_top = self.camera.subscribeCamera("camera_top", 0, 2, 11, 15)

    def onUnload(self):
        self.camera.unsubscribe(self.camera_top)

    def onInput_onStart(self):
        while True:
            image = self.camera.getImageRemote(self.camera_top)
            image_processing(self, image)
            k = cv2.waitKey(30) & 0xff
            if k==27:
                break

    def onInput_onStop(self):
        self.onUnload() #it is recommended to reuse the clean-up as the box is stopped
        self.onStopped() #activate the output of the


    def image_processing(self, image):
        if image == None:
            print "Cannot capture"
            self.camera.closeCamera(0)
            self.camera.stopCamera(0)
            self.tts.say("Working!")
            return
        elif image[6] == None:
            print "No image data string"
            return
        else:
            h = image[0]
            w = image[1]
            image = np.array(image[6])
            image = np.reshape(image, (w, h, 3))
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))
            model.setInput(blob)
            detections = model.forward()

            for i in range(0, detections.shape[2]):
                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
                (startX, startY, endX, endY) = box.astype("int")

                confidence = detections[0, 0, i, 2]

                # If confidence > 0.5, save it as a separate file
                if (confidence > 0.5):
                    frame = image[startY:endY, startX:endX]

                    #Appel du modèle appris pour la detection de masque
                    capture = cv2.resize(frame, (224, 224))
                    capture = capture.reshape((1, capture.shape[0], capture.shape[1], capture.shape[2]))
                    predict = modelMasque.predict(capture)
                    pasDeMasque = predict[0][0]
                    avecMasque = predict[0][1]

                    if (pasDeMasque > avecMasque):
                        cv2.rectangle(image, (startX, startY), (endX, endY),(0, 0, 255), 2)
                        cv2.putText(image, "PAS DE MASQUE", (startX, startY-10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)
                    else:
                        cv2.rectangle(image, (startX, startY), (endX, endY),(0, 255, 0), 2)
                        cv2.putText(image, "OK", (startX, startY), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 255), 2)


            # Affichage de l'image
            cv2.imshow('img', image)
        return]]></content></script><Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" /><Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" /><Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" /><Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" /></Box><Box name="Go to position StandZero" id="2" localization="8" tooltip="Robot will go to the position StandZero&lt;br/&gt;Position description : All motors to 0" x="299" y="43"><bitmap>media/images/positions/StandZero.png</bitmap><script language="4"><content><![CDATA[#~ This script was generated automatically by drang&drop from Position Library
class MyClass(GeneratedClass):
    def __init__(self):
        try: # disable autoBind
          GeneratedClass.__init__(self, False)
        except TypeError: # if NAOqi < 1.14
          GeneratedClass.__init__( self )

    def onLoad(self):
        self.postureProxy = None
        try:
            self.postureProxy = ALProxy("ALRobotPosture")
        except:
            self.logger.error("Module 'ALRobotPosture' not found.")

    def onUnload(self):
        if(self.postureProxy != None):
            self.postureProxy.stopMove()

    def onInput_onStart(self):
        if(self.postureProxy != None):
            result = self.postureProxy.goToPosture("StandZero", 0.8)
            if(result):
                self.success()
            else:
                self.logger.error("Posture StandZero is not a part of the standard posture library or robot cannot reach the posture")
                self.failure()
        else:
            self.failure()

    def onInput_onStop(self):
        self.onUnload() #~ it is recommanded to call onUnload of this box in a onStop method,               as the code written in onUnload is used to stop the box as well
        pass]]></content></script><Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" /><Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" /><Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" /><Output name="success" type="1" type_size="1" nature="1" inner="0" tooltip="Stimulated if the posture has been reached." id="4" /><Output name="failure" type="1" type_size="1" nature="1" inner="0" tooltip="Stimulated if the posture could not be reached." id="5" /><Resource name="All motors" type="Lock" timeout="0" /><Resource name="Stiffness" type="Lock" timeout="0" /></Box><Link inputowner="2" indexofinput="2" outputowner="0" indexofoutput="2" /></Diagram></BehaviorKeyframe></BehaviorLayer></Timeline></Box></ChoregrapheProject>